{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 80px\" src=\"https://raw.githubusercontent.com/trivikverma/researchgroup/master/static/media/resources/epa1316/TU_descriptor%20black.png\"> EPA-1316 Introduction to *Urban* Data Science \n",
    "\n",
    "\n",
    "## Lab 6 Advanced: Advanced Regression Techniques (**Optional** if you have the time to learn more about regression)\n",
    "\n",
    "**TU Delft**<br>\n",
    "**Q1 2020**<br>\n",
    "**Instructor:** Trivik Verma <br>\n",
    "**TAs:** Aarthi Meenakshi Sundaram, Jelle Egbers, Tess Kim, Lotte Lourens, Amir Ebrahimi Fard, Giulia Reggiani, Bramka Jafino, Talia Kaufmann <br>\n",
    "**[Computational Urban Science & Policy Lab](https://research.trivikverma.com/)** <br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<ol start=\"0\">\n",
    "<li> Learning Goals </li>\n",
    "<li> Polynomial Regression with the Cab Data</li>\n",
    "<li> Multiple regression and exploring the Football data </li>\n",
    "<li> A nice trick for forward-backwards </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "After this lab, you should be able to\n",
    "- Explain the difference between train/validation/test data and WHY we have each.\n",
    "- Implement cross-validation on a dataset\n",
    "- Implement arbitrary multiple regression models in both SK-learn and Statsmodels.\n",
    "- Interpret the coefficent estimates produced by each model, including transformed and dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some terminal shortcuts\n",
    "\n",
    "Within your terminal (aka console aka command prompt), most shell environments support useful shortcuts:\n",
    "\n",
    "<ul>\n",
    "  <li>press the [up arrow] to navigate through your most recent commands</li>\n",
    "  <li>press [CTRL + A] to go to the beginning of the line</li>\n",
    "  <li>press [CTRL + E] to go to the end of the line</li>\n",
    "  <li>press [CTRL + K] to clear the line</li>\n",
    "  <li>type `history` to see the last commands you've run</li>\n",
    "</ul>  \n",
    "\n",
    "\n",
    "## Terminology\n",
    "\n",
    "Say we have input features $X$, which via some function $f()$, approximates outputs $Y$. That is, $Y = f(X) + \\epsilon$ (where $\\epsilon$ represents our unmeasurable variation (i.e., irreducible error).\n",
    "\n",
    "- **Inference**: estimates the function $f$, but the goal isn't to make predictions for $Y$; rather, it is more concerned with understanding the relationship between $X$ and $Y$.\n",
    "- **Prediction**: estimates the function $f$ with the goal of making accurate $Y$ predictions for some unseen $X$.\n",
    "\n",
    "\n",
    "We have recently used two highly popular, useful libraries, `statsmodels` and `sklearn`.\n",
    "\n",
    "`statsmodels` is mostly focused on the _inference_ task. It aims to make good estimates for $f()$ (via solving for our $\\beta$'s), and it provides expansive details about its certainty. It provides lots of tools to discuss confidence, but isn't great at dealing with test sets.\n",
    "\n",
    "`sklearn` is mostly focused on the _prediction_ task. It aims to make a well-fit line to our input data $X$, so as to make good $Y$ predictions for some unseen inputs $X$. It provides a shallower analysis of our variables. In other words, `sklearn` is great at test sets and validations, but it can't really discuss uncertainty in the parameters or predictions.\n",
    "\n",
    "\n",
    "- **R-squared**: An interpretable summary of how well the model did. 1 is perfect, 0 is a trivial baseline model based on the mean $y$ value, and negative is worse than the trivial model.\n",
    "- **F-statistic**: A value testing whether we're likely to see these results (or even stronger ones) if none of the predictors actually mattered.\n",
    "- **Prob (F-statistic)**: The probability that we'd see these results (or even stronger ones) if none of the predictors actually mattered. If this probability is small then either A) some combination of predictors actually matters or B) something rather unlikely has happened\n",
    "- **coef**: The estimate of each beta. This has several sub-components:\n",
    "  - **std err**: The amount we'd expect this value to wiggle if we re-did the data collection and re-ran our model. More data tends to make this wiggle smaller, but sometimes the collected data just isn't enough to pin down a particular value.\n",
    "  - **t and P>|t|**: similar to the F-statistic, these measure the probability of seeing coefficients this big (or even bigger) if the given variable didn't actually matter. Small probability doesn't necessarily mean the value matters\n",
    "  - **\\[0.025 0.975\\]**: Endpoints of the 95% confidence interval. This is a interval drawn in a clever way and which gives an idea of where the true beta value might plausibly live.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Polynomial Regression with the Cab Data\n",
    "\n",
    "Polynomial regression uses a **linear model** to estimate a **non-linear function** (i.e., a function with polynomial terms). For example:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1x_i + \\beta_1x_i^{2}$\n",
    "\n",
    "It is a linear model because we are still solving a linear equation (the _linear_ aspect refers to the beta coefficients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# read in the data, break into train and test\n",
    "cab_df = pd.read_csv(\"data/cab.txt\")\n",
    "train_data, test_data = train_test_split(cab_df, test_size=.2, random_state=42)\n",
    "cab_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cab_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some data cleaning\n",
    "X_train = train_data['TimeMin'].values.reshape(-1,1)/60 # transforms it to being hour-based\n",
    "y_train = train_data['PickupCount'].values\n",
    "\n",
    "X_test = test_data['TimeMin'].values.reshape(-1,1)/60 # hour-based\n",
    "y_test = test_data['PickupCount'].values\n",
    "\n",
    "def plot_cabs(cur_model, poly_transformer=None):\n",
    "    \n",
    "    # build the x values for the prediction line\n",
    "    x_vals = np.arange(0,24,.1).reshape(-1,1)\n",
    "    \n",
    "    # optionally use the passed-in transformer\n",
    "    if poly_transformer != None:\n",
    "        dm = poly_transformer.fit_transform(x_vals)\n",
    "    else:\n",
    "        dm = x_vals\n",
    "        \n",
    "    # make the prediction at each x value\n",
    "    prediction = cur_model.predict(dm)\n",
    "    \n",
    "    # plot the prediction line, and the test data\n",
    "    plt.plot(x_vals,prediction, color='k', label=\"Prediction\")\n",
    "    plt.scatter(X_test, y_test, label=\"Test Data\")\n",
    "\n",
    "    # label your plots\n",
    "    plt.ylabel(\"Number of Taxi Pickups\")\n",
    "    plt.xlabel(\"Time of Day (Hours Past Midnight)\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "fitted_cab_model0 = LinearRegression().fit(X_train, y_train)\n",
    "plot_cabs(fitted_cab_model0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fitted_cab_model0.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code uses `sklearn`. As more practice, and to help you stay versed in both libraries, perform the same task (fit a linear regression line) using `statsmodels` and report the $r^2$ score. Is it the same value as what sklearn reports, and is this the expected behavior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment the data with a column vector of 1's\n",
    "train_data_augmented = sm.add_constant(X_train)\n",
    "test_data_augmented = sm.add_constant(X_test)\n",
    "\n",
    "# fit the model on the training data\n",
    "OLSModel = OLS(train_data['PickupCount'].values, train_data_augmented).fit()\n",
    "\n",
    "# get the prediction results\n",
    "ols_predicted_pickups_test = OLSModel.predict(test_data_augmented)\n",
    "r2_score_test = r2_score(test_data[['PickupCount']].values, ols_predicted_pickups_test)\n",
    "print(r2_score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there's still a lot of variation in cab pickups that's not being captured by a linear fit. Further, the linear fit is predicting massively more pickups at 11:59pm than at 12:00am. This is a bad property, and it's the conseqeuence of having a straight line with a non-zero slope. However, we can add columns to our data for $TimeMin^2$ and $TimeMin^3$ and so on, allowing a curvy polynomial line to hopefully fit the data better.\n",
    "\n",
    "We'll be using ``sklearn``'s `PolynomialFeatures()` function to take some of the tedium out of building the expanded input data. In fact, if all we want is a formula like $y \\approx \\beta_0 + \\beta_1 x + \\beta_2 x^2 + ...$, it will directly return a new copy of the data in this format!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "transformer_3 = PolynomialFeatures(3, include_bias=False)\n",
    "expanded_train = transformer_3.fit_transform(X_train) # TRANSFORMS it to polynomial features\n",
    "pd.DataFrame(expanded_train).describe() # notice that the columns now contain x, x^2, x^3 values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few notes on `PolynomialFeatures`:\n",
    "\n",
    "- The interface is a bit strange. `PolynomialFeatures` is a _'transformer'_ in sklearn. We'll be using several transformers that learn a transformation on the training data, and then we will apply those transformations on future data. With PolynomialFeatures, the `.fit()` is pretty trivial, and we often fit and transform in one command, as seen above with ``.fit_transform()`.\n",
    "- You rarely want to `include_bias` (a column of all 1's), since _**sklearn**_ will add it automatically. Remember, when using _**statsmodels,**_ you can just `.add_constant()` right before you fit the data.\n",
    "- If you want polynomial features for a several different variables (i.e., multinomial regression), you should call `.fit_transform()` separately on each column and append all the results to a copy of the data (unless you also want interaction terms between the newly-created features). See `np.concatenate()` for joining arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fitted_cab_model3 = LinearRegression().fit(expanded_train, y_train)\n",
    "print(\"fitting expanded_train:\", expanded_train)\n",
    "plot_cabs(fitted_cab_model3, transformer_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some Questions**:\n",
    "1. Let us calculate the polynomial model's $R^2$ performance on the test set. \n",
    "2. Does the polynomial model improve on the purely linear model?\n",
    "3. Let's make a residual plot for the polynomial model. What does this plot tell us about the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Question1\n",
    "expanded_test = transformer_3.fit_transform(X_test)\n",
    "print(\"Test R-squared:\", fitted_cab_model3.score(expanded_test, y_test))\n",
    "# NOTE 1: unlike statsmodels' r2_score() function, sklearn has a .score() function\n",
    "# NOTE 2: fit_transform() is a nifty function that transforms the data, then fits it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question2 : \n",
    "#What do you think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Question 3\n",
    "x_matrix = transformer_3.fit_transform(X_train)\n",
    "\n",
    "prediction = fitted_cab_model3.predict(x_matrix)\n",
    "residual = y_train - prediction\n",
    "plt.scatter(X_train, residual, label=\"Residual\")\n",
    "plt.axhline(0, color='k')\n",
    "\n",
    "plt.title(\"Residuals for the Cubic Model\")\n",
    "plt.ylabel(\"Residual Number of Taxi Pickups\")\n",
    "plt.xlabel(\"Time of Day (Hours Past Midnight)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other features\n",
    "Polynomial features are not the only constucted features that help fit the data. Because these data have a 24 hour cycle, we may want to build features that follow such a cycle. For example, $sin(24\\frac{x}{2\\pi})$, $sin(12\\frac{x}{2\\pi})$, $sin(8\\frac{x}{2\\pi})$. Other feature transformations are appropriate to other types of data. For instance certain feature transformations have been developed for geographical data.\n",
    "\n",
    "### Scaling Features\n",
    "When using polynomials, we are explicitly trying to use the higher-order values for a given feature. However, sometimes these polynomial features can take on values that are drastically large, making it difficult for the system to learn an appropriate bias weight due to its large values and potentially large variance. To counter this, sometimes one may be interested in scaling the values for a given feature.\n",
    "\n",
    "For our ongoing taxi-pickup example, using polynomial features improved our model. If we wished to scale the features, we could use `sklearn`'s StandardScaler() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# SCALES THE EXPANDED/POLY TRANSFORMED DATA\n",
    "# we don't need to convert to a pandas dataframe, but it can be useful for scaling select columns\n",
    "train_copy = pd.DataFrame(expanded_train.copy())\n",
    "test_copy = pd.DataFrame(expanded_test.copy())\n",
    "\n",
    "# Fit the scaler on the training data\n",
    "scaler = StandardScaler().fit(train_copy)\n",
    "\n",
    "# Scale both the test and training data. \n",
    "train_scaled = scaler.transform(expanded_train)\n",
    "test_scaled = scaler.transform(expanded_test)\n",
    "\n",
    "# we could optionally run a new regression model on this scaled data\n",
    "fitted_scaled_cab = LinearRegression().fit(train_scaled, y_train)\n",
    "fitted_scaled_cab.score(test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px\">\n",
    "\n",
    "## Part 2: Multiple regression and exploring the Football (aka soccer) data\n",
    "Let's move on to a different dataset! The data imported below were scraped by [Shubham Maurya](https://www.kaggle.com/mauryashubham/linear-regression-to-predict-market-value/data) and record various facts about players in the English Premier League. Our goal will be to fit models that predict the players' market value (what the player could earn when hired by a new team), as estimated by https://www.transfermarkt.us.\n",
    "\n",
    "`name`: Name of the player  \n",
    "`club`: Club of the player  \n",
    "`age` : Age of the player  \n",
    "`position` : The usual position on the pitch  \n",
    "`position_cat` :  1 for attackers, 2 for midfielders, 3 for defenders, 4 for goalkeepers  \n",
    "`market_value` : As on transfermrkt.com on July 20th, 2017  \n",
    "`page_views` : Average daily Wikipedia page views from September 1, 2016 to May 1, 2017  \n",
    "`fpl_value` : Value in Fantasy Premier League as on July 20th, 2017  \n",
    "`fpl_sel` : % of FPL players who have selected that player in their team  \n",
    "`fpl_points` : FPL points accumulated over the previous season  \n",
    "`region`: 1 for England, 2 for EU, 3 for Americas, 4 for Rest of World  \n",
    "`nationality`: Player's nationality  \n",
    "`new_foreign`: Whether a new signing from a different league, for 2017/18 (till 20th July)  \n",
    "`age_cat`: a categorical version of the Age feature  \n",
    "`club_id`: a numerical version of the Club feature  \n",
    "`big_club`: Whether one of the Top 6 clubs  \n",
    "`new_signing`: Whether a new signing for 2017/18 (till 20th July)  \n",
    "\n",
    "As always, we first import, verify, split, and explore the data.\n",
    "\n",
    "## Part 2.1: Import and verification and grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "league_df = pd.read_csv(\"data/league_data.txt\")\n",
    "print(league_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "league_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "league_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Stratified) train/test split\n",
    "We want to make sure that the training and test data have appropriate representation of each region; it would be bad for the training data to entirely miss a region. This is especially important because some regions are rather rare.\n",
    "\n",
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "\n",
    "**Questions**:\n",
    "1. Use the `train_test_split()` function, while (a) ensuring the test size is 20% of the data, and; (2) using 'stratify' argument to split the data (look up documentation online), keeping equal representation of each region. This doesn't work by default, correct? What is the issue?\n",
    "2. Deal with the issue you encountered above. Hint: you may find numpy's `.isnan()` and panda's `.dropna()` functions useful!\n",
    "3. How did you deal with the error generated by `train_test_split`? How did you justify your action? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: feel free to work with someone\n",
    "#Part1: \n",
    "train_data, test_data = train_test_split(league_df, test_size = 0.2, stratify=league_df['region'])\n",
    "\n",
    "#Part2:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#after processing: \n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we won't be peeking at the test set, let's explore and look for patterns! We'll introduce a number of useful pandas and numpy functions along the way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby\n",
    "Pandas' `.groupby()` function is a wonderful tool for data analysis. It allows us to analyze each of several subgroups.\n",
    "\n",
    "Many times, `.groupby()` is combined with `.agg()` to get a summary statistic for each subgroup. For instance: What is the average market value, median page views, and maximum fpl for each player position?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_data.groupby('position').agg({\n",
    "    'market_value': np.mean,\n",
    "    'page_views': np.median,\n",
    "    'fpl_points': np.max\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_data.position.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_data.groupby(['big_club', 'position']).agg({\n",
    "    'market_value': np.mean,\n",
    "    'page_views': np.mean,\n",
    "    'fpl_points': np.mean\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "\n",
    "**Question**:\n",
    "1. Notice that the `.groupby()` function above takes a list of two column names. Does the order matter? What happens if we switch the two so that 'position' is listed before 'big_club'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here, switching the two columns: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in this case, our values are the same, as we are not aggregating anything differently.\n",
    "However, our view / grouping is merely different. visually, it often makes most sense to group such that the left-most (earlier) groupings have fewer distinct options than the ones to the right of it, but it all depends on what you're trying to discern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px\">\n",
    "\n",
    "## Part 2.2: Linear regression on the football data\n",
    "This section of the lab focuses on fitting a model to the football (soccer) data and interpreting the model results. The model we'll use is\n",
    "\n",
    "$$\\text{market_value} \\approx \\beta_0 + \\beta_1\\text{fpl_points} + \\beta_2\\text{age} + \\beta_3\\text{age}^2 + \\beta_4log_2\\left(\\text{page_views}\\right) + \\beta_5\\text{new_signing} +\\beta_6\\text{big_club} + \\beta_7\\text{position_cat}$$\n",
    "\n",
    "We're including a 2nd degree polynomial in age because we expect pay to increase as a player gains experience, but then decrease as they continue aging. We're taking the log of page views because they have such a large, skewed range and the transformed variable will have fewer outliers that could bias the line. We choose the base of the log to be 2 just to make interpretation cleaner.\n",
    "\n",
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "\n",
    "**Steps and some questions you can ask at each stage...**:\n",
    "1. Build the data and fit this model to it. How good is the overall model?\n",
    "2. Interpret the regression model. What is the meaning of the coefficient for:\n",
    "    - age and age$^2$\n",
    "    - $log_2($page_views$)$\n",
    "    - big_club\n",
    "3. What should a player do in order to improve their market value? How many page views should a player go get to increase their market value by 10?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Part 1:\n",
    "y_train = train_data['market_value']\n",
    "y_test = test_data['market_value']\n",
    "def build_football_data(df):\n",
    "    x_matrix = df[['fpl_points','age','new_signing','big_club','position_cat']].copy()\n",
    "    x_matrix['log_views'] = np.log2(df['page_views'])\n",
    "    \n",
    "    # WRITE CODE FOR CREATING THE AGE SQUARED COLUMN\n",
    "    ####\n",
    "    \n",
    "    # OPTIONALLY WRITE CODE to adjust the ordering of the columns, just so that it corresponds with the equation above\n",
    "    ####\n",
    "    \n",
    "    # add a constant\n",
    "    x_matrix = sm.add_constant(x_matrix)\n",
    "    \n",
    "    return x_matrix\n",
    "\n",
    "# use build_football_data() to transform both the train_data and test_data\n",
    "train_transformed = build_football_data(train_data)\n",
    "test_transformed = build_football_data(test_data)\n",
    "\n",
    "fitted_model_1 = OLS(endog= y_train, exog=train_transformed, hasconst=True).fit()\n",
    "fitted_model_1.summary()\n",
    "\n",
    "# CODE TO RUN r2_score(), then answer the above question about the overall goodness of the model\n",
    "r2_score(y_test, fitted_model_1.predict(test_transformed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Part2: let's use the age coefficients to show the effect of age has on one's market value;\n",
    "# we can get the age and age^2 coefficients via:\n",
    "agecoef = fitted_model_1.params.age\n",
    "age2coef = fitted_model_1.params.age_squared\n",
    "\n",
    "# let's set our x-axis (corresponding to age) to be a wide range from -100 to 100, \n",
    "# just to see a grand picture of the function\n",
    "x_vals = np.linspace(-100,100,1000)\n",
    "y_vals = agecoef*x_vals +age2coef*x_vals**2\n",
    "\n",
    "# PLOT x_vals vs y_vals\n",
    "plt.plot(x_vals, y_vals)\n",
    "plt.title(\"Effect of Age\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Contribution to Predicted Market Value\")\n",
    "plt.show()\n",
    "\n",
    "# Q2A: WHAT HAPPENS IF WE USED ONLY AGE (not AGE^2) in our model (what's the r2?); make the same plot of age vs market value\n",
    "\n",
    "# Q2B: WHAT HAPPENS IF WE USED ONLY AGE^2 (not age) in our model (what's the r2?); make the same plot of age^2 vs market value\n",
    "\n",
    "# PLOT page views vs market value\n",
    "page_view_coef = fitted_model_1.params.log_views\n",
    "x_vals = np.linspace(0,15)\n",
    "y_vals = page_view_coef*x_vals\n",
    "plt.plot(x_vals, y_vals)\n",
    "plt.title(\"Effect of Page Views\")\n",
    "plt.xlabel(\"Page Views\")\n",
    "plt.ylabel(\"Contribution to Predicted Market Value\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style='height:3px'>\n",
    "\n",
    "### Part 2.3: Turning Categorical Variables into multiple binary variables\n",
    "Of course, we have an error in how we've included player position. Even though the variable is numeric (1,2,3,4) and the model runs without issue, the value we're getting back is garbage. The interpretation, such as it is, is that there is an equal effect of moving from position category 1 to 2, from 2 to 3, and from 3 to 4, and that this effect is probably between -0.5 to -1 (depending on your run).\n",
    "\n",
    "In reality, we don't expect moving from one position category to another to be equivalent, nor for a move from category 1 to category 3 to be twice as important as a move from category 1 to category 2. We need to introduce better features to model this variable.\n",
    "\n",
    "We'll use `pd.get_dummies` to do the work for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_design_recoded = pd.get_dummies(train_transformed, columns=['position_cat'], drop_first=True)\n",
    "test_design_recoded = pd.get_dummies(test_transformed, columns=['position_cat'], drop_first=True)\n",
    "\n",
    "train_design_recoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've removed the original `position_cat` column and created three new ones.\n",
    "\n",
    "#### Why only three new columns?\n",
    "Why does pandas give us the option to drop the first category? \n",
    "\n",
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "\n",
    "**Questions**:\n",
    "1. If we're fitting a model without a constant, should we have three dummy columns or four dummy columns?\n",
    "2. Fit a model on the new, recoded data, then interpret the coefficient of `position_cat_2`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resu = OLS(y_train, train_design_recoded).fit()\n",
    "resu.summary()\n",
    "print(\"r2:\", r2_score(y_test, resu.predict(test_design_recoded)))\n",
    "print(\"position_cat_2 coef:\", resu.params.position_cat_2)\n",
    "train_design_recoded.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: A nice trick for forward-backwards\n",
    "\n",
    "XOR (operator ^) is a logical operation that only returns true when input differ. We can use it to implement forward-or-backwards selection when we want to keep track of whet predictors are \"left\" from a given list of predictors.\n",
    "\n",
    "The set analog is \"symmetric difference\". From the python docs:\n",
    "\n",
    "`s.symmetric_difference(t)\ts ^ t\tnew set with elements in either s or t but not both`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set() ^ set([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([1]) ^ set([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([1, 2]) ^ set([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework Exercise:\n",
    "We have provided a spreadsheet of Boston housing prices (data/boston_housing.csv). The 14 columns are as follows:\n",
    "1. CRIM: per capita crime rate by town\n",
    "2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "3. INDUS: proportion of non-retail business acres per town\n",
    "4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "5. NOX: nitric oxides concentration (parts per 10 million)\n",
    "6. RM: average number of rooms per dwelling\n",
    "7. AGE: proportion of owner-occupied units built prior to 1940\n",
    "8. DIS: weighted distances to ﬁve Boston employment centers\n",
    "9. RAD: index of accessibility to radial highways\n",
    "10. TAX: full-value property-tax rate per \\$10,000\n",
    "11. PTRATIO: pupil-teacher ratio by town\n",
    "12. B: 1000(Bk−0.63)2 where Bk is the proportion of blacks by town\n",
    "13. LSTAT: % lower status of the population\n",
    "14. MEDV: Median value of owner-occupied homes in $1000s We can see that the input attributes have a mixture of units\n",
    "\n",
    "There are 450 observations.\n",
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "\n",
    "Using the above file, try your best to predict **housing prices. (the 14th column)** We have provided a test set `data/boston_housing_test.csv` but refrain from looking at the file or evaluating on it until you have finalized and trained a model.\n",
    "1. Load in the data. It is tab-delimited. Quickly look at a summary of the data to familiarize yourself with it and ensure nothing is too egregious.\n",
    "2. Use a previously-discussed function to automatically partition the data into a training and validation (aka development) set. It is up to you to choose how large these two portions should be.\n",
    "3. Train a basic model on just a subset of the features. What is the performance on the validation set?\n",
    "4. Train a basic model on all of the features. What is the performance on the validation set?\n",
    "5. Toy with the model until you feel your results are reasonably good.\n",
    "6. Perform cross-validation with said model, and measure the average performance. Are the results what you expected? Were the average results better or worse than that from your original 1 validation set?\n",
    "7. Experiment with other models, and for each, perform 10-fold cross-validation. Which model yields the best average performance? Select this as your final model.\n",
    "8. Use this model to evaulate your performance on the testing set. What is your performance (MSE)? Is this what you expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
