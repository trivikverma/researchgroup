{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 80px\" src=\"https://raw.githubusercontent.com/trivikverma/researchgroup/master/static/media/resources/epa1316/TU_descriptor%20black.png\"> EPA-1316 Introduction to *Urban* Data Science \n",
    "\n",
    "\n",
    "## Lab 1_Extra: Web Scraping\n",
    "\n",
    "**TU Delft**<br>\n",
    "**Q1 2020**<br>\n",
    "**Instructor:** Trivik Verma <br>\n",
    "**TAs:** Aarthi Meenakshi Sundaram, Jelle Egbers, Tess Kim, Lotte Lourens, Amir Ebrahimi Fard, Giulia Reggiani, Bramka Jafino <br>\n",
    "**[Computational Urban Science & Policy Lab](https://research.trivikverma.com/)** <br>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents \n",
    "<ol start=\"0\">\n",
    "<li> Learning Goals </li>\n",
    "<li> Introduction to Web Servers and HTTP </li>\n",
    "<li> Download webpages and get basic properties </li>\n",
    "<li> Parse the page with Beautiful Soup</li>\n",
    "<li> String formatting</li>\n",
    "<li> Additonal Python/Homework Comment</li>\n",
    "<li> Walkthrough Example</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Goals\n",
    "\n",
    "- Understand the structure of a web page\n",
    "- Understand how to use Beautiful soup to scrape content from web pages.\n",
    "- Feel comfortable storing and manipulating the content in various formats.\n",
    "- Understand how to convert structured format into a Pandas DataFrame\n",
    "\n",
    "In this lab, we'll scrape Goodread's Best Books list:\n",
    "\n",
    "https://www.goodreads.com/list/show/1.Best_Books_Ever?page=1 .\n",
    "\n",
    "We'll walk through scraping the list pages for the book names/urls. First, we start with an even simpler example.\n",
    "\n",
    "*This lab corresponds to lectures #2 and #3 and maps on to Homework #1 and further.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to Web Servers and HTTP\n",
    "\n",
    "A web server is just a computer -- usually a powerful one, but ultimately it's just another computer -- that runs a long/continuous process that listens for requests on a pre-specified (Internet) _port_ on your computer. It responds to those requests via a protocol called HTTP (HyperText Transfer Protocol). HTTPS is the secure version. When we use a web browser and navigate to a web page, our browser is actually sending a request on our behalf to a specific web server. The browser request is essentially saying \"hey, please give me the web page contents\", and it's up to the browser to correctly render that raw content into a coherent manner, dependent on the format of the file. For example, HTML is one format, XML is another format, and so on.\n",
    "\n",
    "Ideally (and usually), the web server complies with the request and all is fine. As part of this communication exchange with web servers, the server also sends a status code.\n",
    "- If the code starts with a **2**, it means the request was successful.\n",
    "- If the code starts with a **4**, it means there was a client error (you, as the user, are the client). For example, ever receive a 404 File Not Found error because a web page doesn't exist? This is an example of a client error, because you are requesting a bogus item.\n",
    "- If the code starts with a **5**, it means there was a server error (often that your request was incorrectly formed).\n",
    "\n",
    "[Click here](https://www.restapitutorial.com/httpstatuscodes.html) for a full list of status codes.\n",
    "\n",
    "As an analogy, you can think of a web server as being like a server at a restaurant; its goal is _serve_ you your requests. When you try to order something not on the menu (i.e., ask for a web page at a wrong location), the server says 'sorry, we don't have that' (i.e., 404, client error; your mistake).\n",
    "\n",
    "**IMPORTANT:**\n",
    "As humans, we visit pages in a sane, reasonable rate. However, as we start to scrape web pages with our computers, we will be sending requests with our code, and thus, we can make requests at an incredible rate. This is potentially dangerous because it's akin to going to a restaurant and bombarding the server(s) with thousands of food orders. Very often, the restaurant will ban you (i.e., Harvard's network gets banned from the website, and you are potentially held responsible in some capacity?). It is imperative to be responsible and careful. In fact, this act of flooding web pages with requests is the single-most popular, yet archiac, method for maliciously attacking websites / computers with Internet connections. In short, be respectful and careful with your decisions and code. It is better to err on the side of caution, which includes using the **``time.sleep()`` function** to pause your code's execution between subsequent requests. ``time.sleep(2)`` should be fine when making just a few dozen requests. Each site has its own rules, which are often visible via their site's ``robots.txt`` file.\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "**HTML:** if you are not familiar with HTML see https://www.w3schools.com/html/ or one of the many tutorials on the internet.\n",
    "\n",
    "**Document Object Model (DOM):** for more on this programming interface for HTML and XML documents see https://www.w3schools.com/js/js_htmldom.asp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Download webpages and get basic properties\n",
    "\n",
    "``Requests`` is a highly useful Python library that allows us to fetch web pages.\n",
    "``BeautifulSoup`` is a phenomenal Python library that allows us to easily parse web content and perform basic extraction.\n",
    "\n",
    "If one wishes to scrape webpages, one usually uses ``requests`` to fetch the page and ``BeautifulSoup`` to parse the page's meaningful components. Webpages can be messy, despite having a structured format, which is why BeautifulSoup is so handy.\n",
    "\n",
    "Let's get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fetch a webpage's content, we can simply use the ``get()`` function within the requests library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.npr.org/2018/11/05/664395755/what-if-the-polls-are-wrong-again-4-scenarios-for-what-might-happen-in-the-elect\"\n",
    "response = requests.get(url) # you can use any URL that you wish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response variable has many highly useful attributes, such as:\n",
    "- status_code\n",
    "- text\n",
    "- content\n",
    "\n",
    "Let's try each of them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have received a status code of 200, which means the page was successfully found on the server and sent to receiver (aka client/user/you). [Again, you can click here](https://www.restapitutorial.com/httpstatuscodes.html) for a full list of status codes.\n",
    "\n",
    "### response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html lang=\"en\">\\n<head>\\n    <meta name=\"robots\" content=\"noindex, nofollow\">\\n    <meta content=\"text/html;charset=utf-8\" http-equiv=\"Content-Type\">\\n    <meta content=\"utf-8\" http-equiv=\"encoding\">\\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1, minimum-scale=1, maximum-scale=1, shrink-to-fit=no\" />\\n\\n    <title>NPR Choice page</title>\\n\\n    <link rel=\"stylesheet\" media=\"screen, print\" href=\"https://s.npr.org/templates/css/fonts/Knockout.css\"/>\\n    <link rel=\"stylesheet\" media=\"screen, print\" href=\"https://s.npr.org/templates/css/fonts/GothamSSm.css\"/>\\n    <link rel=\"stylesheet\" media=\"screen, print\" href=\"css/choice-stylesheet.css\"/>\\n    <script type=\"text/javascript\" src=\"./js/redirects.js\"></script>\\n    <script type=\"text/javascript\" src=\"./js/domains.js\"></script>\\n</head>\\n<body>\\n<main class=\"content\" id=\"content\">\\n    <header role=\"banner\">\\n        <img src=\"https://media.npr.org/chrome_svg/npr-logo.svg\" alt=\"NPR logo\" class=\"npr-logo\"/>\\n\\n        <h1 class=\"header-txt\">Data Protection Choices</h1>\\n\\n        <div id=\"npr-rule\" role=\"presentation\"><span></span><span></span></div>\\n    </header>\\n\\n    <section class=\"main-section\">\\n        <p>\\n            By choosing &ldquo;I agree&rdquo; below, you agree that NPR&rsquo;s sites use cookies, similar tracking and storage technologies, and information about the device you use to access our sites to enhance your viewing, listening and user experience, personalize content, personalize messages from NPR&rsquo;s sponsors, provide social media features, and analyze NPR&rsquo;s traffic. This information is shared with social media services, sponsorship, analytics and other third-party service providers.\\n            <a href=\"https://text.npr.org/s.php?sId=609131973#cookiepolicy\">See details</a>.\\n        </p>\\n\\n        <p class=\"acceptance-date\" id=\"acceptanceDate\"></p>\\n\\n        <div class=\"user-actions\">\\n            <button class=\"user-action user-action--accept\" id=\"accept\">Agree and Continue</button>\\n\\n            <button class=\"user-action user-action--revoke\" id=\"revoke\">Revoke Agreement</button>\\n\\n            <a class=\"user-action user-action--text\" id=\"textLink\" href=\"https://text.npr.org\">Decline and Visit Plain Text Site</a>\\n        </div>\\n\\n        <footer class=\"footer\">\\n            <p>NPR&rsquo;s <a href=\"https://text.npr.org/s.php?sId=179876898\">Terms of Use</a> and <a\\n                    href=\"https://text.npr.org/s.php?sId=609131973\">Privacy Policy</a>.</p>\\n        </footer>\\n    </section>\\n</main>\\n\\n<script>\\n    // self executing function here\\n    (function () {\\n        var choiceVersion = 1;\\n\\n        // Return true is the origin param is present in the URL\\n        // Make sure origin starts with \"https://\" in order to avoid cross-site scripting attack\\n        var hasOrigin = function () {\\n            var searchParam = window.location.search;\\n            return searchParam.substr(0, 16) === \\'?origin=https://\\';\\n        };\\n\\n        // Append choiceRedirect=true to a destination\\n        // This will tell use that a user has been already redirected by the choice page\\n        // stopping a potential infinite redirect loop \\n        var addChoiceRedirectParam = function (url) {\\n            var paramControl = \\'?\\';\\n            if (url.includes(\\'?\\')){\\n                paramControl = \\'&\\';\\n            }\\n            return url + paramControl + \\'t=\\' + (new Date()).getTime();\\n        }\\n\\n        // Redirect made from AKAMAI will include the original\\n        // destination with the request ex:\\n        // https://www.npr.org/choice.html?origin=https://www.npr.org/about-npr/178660742/public-radio-finances\\n        var getDestination = function () {\\n            var searchParam = window.location.search;\\n            if (hasOrigin()) {\\n                var destination = searchParam.substr(8);\\n                if (checkOrigin(destination)) {\\n                    return destination;\\n                } \\n            }\\n            return \\'https://www.npr.org\\';\\n        };\\n\\n        var getCookie = function (name) {\\n            var value = \"; \" + document.cookie;\\n            var parts = value.split(\"; \" + name + \"=\");\\n            if (parts.length == 2) return parts.pop().split(\";\").shift();\\n            return false;\\n        };\\n\\n        var delete_cookie = function (name) {\\n            document.cookie = name + \\'=;expires=Thu, 01 Jan 1970 00:00:01 GMT;secure;path=/;domain=npr.org;\\';\\n        };\\n\\n        var create_cookie = function (name, value) {\\n            // Cookies have a tendency to expire, so I arbitrarily set the max age to 10 year\\n            document.cookie = name + \\'=\\' + value + \\';secure;path=/;domain=npr.org;max-age=315360000;\\';\\n        };\\n\\n        // True is user previously accepted the correct version of the consent page\\n        var hasPreviouslyAcceptedChoiceOptions = function () {\\n            return getCookie(\\'trackingChoice\\') && getCookie(\\'choiceVersion\\') == choiceVersion;\\n        }\\n\\n        // Grab the thing id form the destination\\n        var getThingId = function (destination) {\\n            var yearMonthDateWithPreFixReg = /https:\\\\/\\\\/www\\\\.npr\\\\.org\\\\/([a-z]+\\\\/){0,2}\\\\d{4}\\\\/\\\\d{2}\\\\/\\\\d{2}\\\\/(\\\\d+)\\\\/.*/;\\n            var match = yearMonthDateWithPreFixReg.exec(destination);\\n            if (match) {\\n                return match[2];\\n            }\\n\\n            var noDateUrlRegex = /https:\\\\/\\\\/www\\\\.npr\\\\.org\\\\/([a-z]+\\\\/){1,2}(\\\\d+)\\\\/.*/;\\n            match = noDateUrlRegex.exec(destination);\\n            if (match) {\\n                return match[2];\\n            }\\n\\n            var thingIdByParam = /https:\\\\/\\\\/www\\\\.npr\\\\.org\\\\/.*[iI]d=(\\\\d{4,}).*/;\\n            match = thingIdByParam.exec(destination);\\n            if (match) {\\n                return match[1];\\n            }\\n\\n            // Check if we have a hard coded page url\\n            //Remove https://www.npr.org from the destination\\n            var location = destination.substr(19);\\n            for (var key in redirectLookup) {\\n                // If the first part of the location matches a\\n                // hard coded url, then we have a match.\\n                if (location.startsWith(key)){\\n                    return redirectLookup[key];\\n                }\\n            }\\n\\n            return false;\\n        }\\n\\n        document.getElementById(\\'accept\\').addEventListener(\\'click\\', function () {\\n            var d = new Date();\\n            var dateOfChoice = d.getTime();\\n\\n            create_cookie(\\'trackingChoice\\', \\'true\\');\\n            create_cookie(\\'choiceVersion\\', choiceVersion);\\n            create_cookie(\\'dateOfChoice\\', dateOfChoice);\\n            window.location = addChoiceRedirectParam(getDestination());\\n        });\\n\\n        document.getElementById(\\'revoke\\').addEventListener(\\'click\\', function () {\\n            delete_cookie(\\'trackingChoice\\');\\n            delete_cookie(\\'choiceVersion\\');\\n            delete_cookie(\\'dateOfChoice\\');\\n            document.getElementById(\\'acceptanceDate\\').innerText = \\'\\';\\n            document.getElementById(\\'content\\').classList.remove(\\'accepted\\');\\n        });\\n\\n\\n        var thingId = getThingId(getDestination());\\n        if (thingId) {\\n            document.getElementById(\\'textLink\\').href = \"https://text.npr.org/r.php?id=\" + thingId;\\n        }\\n\\n        if (hasOrigin() && hasPreviouslyAcceptedChoiceOptions()) {\\n            // If the user has already accepted the choice options\\n            // and has an origin param in his request\\n            // We will redirect him to that origin request.\\n            // This will solve the issue where applications are caching 307 redirects\\n            window.location = addChoiceRedirectParam(getDestination());\\n        } else if (hasPreviouslyAcceptedChoiceOptions()) {\\n            var lastDateOfChoice = getCookie(\\'dateOfChoice\\');\\n            var d = new Date(parseInt(lastDateOfChoice, 10));\\n            var dateString = \"On \"\\n                + (d.getMonth() + 1)\\n                + \"/\"\\n                + (d.getDay() + 1)\\n                + \"/\"\\n                + d.getFullYear()\\n                + \" you accepted to the above.\";\\n            document.getElementById(\\'acceptanceDate\\').innerText = dateString;\\n            document.getElementById(\\'content\\').classList.add(\\'accepted\\');\\n        }\\n\\n    })();\\n</script>\\n</body>\\n</html>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holy moly! That looks awful. If we use our browser to visit the URL, then right-click the page and click 'View Page Source', we see that it is identical to this chunk of glorious text.\n",
    "\n",
    "### response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!DOCTYPE html>\\n<html lang=\"en\">\\n<head>\\n    <meta name=\"robots\" content=\"noindex, nofollow\">\\n    <meta content=\"text/html;charset=utf-8\" http-equiv=\"Content-Type\">\\n    <meta content=\"utf-8\" http-equiv=\"encoding\">\\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1, minimum-scale=1, maximum-scale=1, shrink-to-fit=no\" />\\n\\n    <title>NPR Choice page</title>\\n\\n    <link rel=\"stylesheet\" media=\"screen, print\" href=\"https://s.npr.org/templates/css/fonts/Knockout.css\"/>\\n    <link rel=\"stylesheet\" media=\"screen, print\" href=\"https://s.npr.org/templates/css/fonts/GothamSSm.css\"/>\\n    <link rel=\"stylesheet\" media=\"screen, print\" href=\"css/choice-stylesheet.css\"/>\\n    <script type=\"text/javascript\" src=\"./js/redirects.js\"></script>\\n    <script type=\"text/javascript\" src=\"./js/domains.js\"></script>\\n</head>\\n<body>\\n<main class=\"content\" id=\"content\">\\n    <header role=\"banner\">\\n        <img src=\"https://media.npr.org/chrome_svg/npr-logo.svg\" alt=\"NPR logo\" class=\"npr-logo\"/>\\n\\n        <h1 class=\"header-txt\">Data Protection Choices</h1>\\n\\n        <div id=\"npr-rule\" role=\"presentation\"><span></span><span></span></div>\\n    </header>\\n\\n    <section class=\"main-section\">\\n        <p>\\n            By choosing &ldquo;I agree&rdquo; below, you agree that NPR&rsquo;s sites use cookies, similar tracking and storage technologies, and information about the device you use to access our sites to enhance your viewing, listening and user experience, personalize content, personalize messages from NPR&rsquo;s sponsors, provide social media features, and analyze NPR&rsquo;s traffic. This information is shared with social media services, sponsorship, analytics and other third-party service providers.\\n            <a href=\"https://text.npr.org/s.php?sId=609131973#cookiepolicy\">See details</a>.\\n        </p>\\n\\n        <p class=\"acceptance-date\" id=\"acceptanceDate\"></p>\\n\\n        <div class=\"user-actions\">\\n            <button class=\"user-action user-action--accept\" id=\"accept\">Agree and Continue</button>\\n\\n            <button class=\"user-action user-action--revoke\" id=\"revoke\">Revoke Agreement</button>\\n\\n            <a class=\"user-action user-action--text\" id=\"textLink\" href=\"https://text.npr.org\">Decline and Visit Plain Text Site</a>\\n        </div>\\n\\n        <footer class=\"footer\">\\n            <p>NPR&rsquo;s <a href=\"https://text.npr.org/s.php?sId=179876898\">Terms of Use</a> and <a\\n                    href=\"https://text.npr.org/s.php?sId=609131973\">Privacy Policy</a>.</p>\\n        </footer>\\n    </section>\\n</main>\\n\\n<script>\\n    // self executing function here\\n    (function () {\\n        var choiceVersion = 1;\\n\\n        // Return true is the origin param is present in the URL\\n        // Make sure origin starts with \"https://\" in order to avoid cross-site scripting attack\\n        var hasOrigin = function () {\\n            var searchParam = window.location.search;\\n            return searchParam.substr(0, 16) === \\'?origin=https://\\';\\n        };\\n\\n        // Append choiceRedirect=true to a destination\\n        // This will tell use that a user has been already redirected by the choice page\\n        // stopping a potential infinite redirect loop \\n        var addChoiceRedirectParam = function (url) {\\n            var paramControl = \\'?\\';\\n            if (url.includes(\\'?\\')){\\n                paramControl = \\'&\\';\\n            }\\n            return url + paramControl + \\'t=\\' + (new Date()).getTime();\\n        }\\n\\n        // Redirect made from AKAMAI will include the original\\n        // destination with the request ex:\\n        // https://www.npr.org/choice.html?origin=https://www.npr.org/about-npr/178660742/public-radio-finances\\n        var getDestination = function () {\\n            var searchParam = window.location.search;\\n            if (hasOrigin()) {\\n                var destination = searchParam.substr(8);\\n                if (checkOrigin(destination)) {\\n                    return destination;\\n                } \\n            }\\n            return \\'https://www.npr.org\\';\\n        };\\n\\n        var getCookie = function (name) {\\n            var value = \"; \" + document.cookie;\\n            var parts = value.split(\"; \" + name + \"=\");\\n            if (parts.length == 2) return parts.pop().split(\";\").shift();\\n            return false;\\n        };\\n\\n        var delete_cookie = function (name) {\\n            document.cookie = name + \\'=;expires=Thu, 01 Jan 1970 00:00:01 GMT;secure;path=/;domain=npr.org;\\';\\n        };\\n\\n        var create_cookie = function (name, value) {\\n            // Cookies have a tendency to expire, so I arbitrarily set the max age to 10 year\\n            document.cookie = name + \\'=\\' + value + \\';secure;path=/;domain=npr.org;max-age=315360000;\\';\\n        };\\n\\n        // True is user previously accepted the correct version of the consent page\\n        var hasPreviouslyAcceptedChoiceOptions = function () {\\n            return getCookie(\\'trackingChoice\\') && getCookie(\\'choiceVersion\\') == choiceVersion;\\n        }\\n\\n        // Grab the thing id form the destination\\n        var getThingId = function (destination) {\\n            var yearMonthDateWithPreFixReg = /https:\\\\/\\\\/www\\\\.npr\\\\.org\\\\/([a-z]+\\\\/){0,2}\\\\d{4}\\\\/\\\\d{2}\\\\/\\\\d{2}\\\\/(\\\\d+)\\\\/.*/;\\n            var match = yearMonthDateWithPreFixReg.exec(destination);\\n            if (match) {\\n                return match[2];\\n            }\\n\\n            var noDateUrlRegex = /https:\\\\/\\\\/www\\\\.npr\\\\.org\\\\/([a-z]+\\\\/){1,2}(\\\\d+)\\\\/.*/;\\n            match = noDateUrlRegex.exec(destination);\\n            if (match) {\\n                return match[2];\\n            }\\n\\n            var thingIdByParam = /https:\\\\/\\\\/www\\\\.npr\\\\.org\\\\/.*[iI]d=(\\\\d{4,}).*/;\\n            match = thingIdByParam.exec(destination);\\n            if (match) {\\n                return match[1];\\n            }\\n\\n            // Check if we have a hard coded page url\\n            //Remove https://www.npr.org from the destination\\n            var location = destination.substr(19);\\n            for (var key in redirectLookup) {\\n                // If the first part of the location matches a\\n                // hard coded url, then we have a match.\\n                if (location.startsWith(key)){\\n                    return redirectLookup[key];\\n                }\\n            }\\n\\n            return false;\\n        }\\n\\n        document.getElementById(\\'accept\\').addEventListener(\\'click\\', function () {\\n            var d = new Date();\\n            var dateOfChoice = d.getTime();\\n\\n            create_cookie(\\'trackingChoice\\', \\'true\\');\\n            create_cookie(\\'choiceVersion\\', choiceVersion);\\n            create_cookie(\\'dateOfChoice\\', dateOfChoice);\\n            window.location = addChoiceRedirectParam(getDestination());\\n        });\\n\\n        document.getElementById(\\'revoke\\').addEventListener(\\'click\\', function () {\\n            delete_cookie(\\'trackingChoice\\');\\n            delete_cookie(\\'choiceVersion\\');\\n            delete_cookie(\\'dateOfChoice\\');\\n            document.getElementById(\\'acceptanceDate\\').innerText = \\'\\';\\n            document.getElementById(\\'content\\').classList.remove(\\'accepted\\');\\n        });\\n\\n\\n        var thingId = getThingId(getDestination());\\n        if (thingId) {\\n            document.getElementById(\\'textLink\\').href = \"https://text.npr.org/r.php?id=\" + thingId;\\n        }\\n\\n        if (hasOrigin() && hasPreviouslyAcceptedChoiceOptions()) {\\n            // If the user has already accepted the choice options\\n            // and has an origin param in his request\\n            // We will redirect him to that origin request.\\n            // This will solve the issue where applications are caching 307 redirects\\n            window.location = addChoiceRedirectParam(getDestination());\\n        } else if (hasPreviouslyAcceptedChoiceOptions()) {\\n            var lastDateOfChoice = getCookie(\\'dateOfChoice\\');\\n            var d = new Date(parseInt(lastDateOfChoice, 10));\\n            var dateString = \"On \"\\n                + (d.getMonth() + 1)\\n                + \"/\"\\n                + (d.getDay() + 1)\\n                + \"/\"\\n                + d.getFullYear()\\n                + \" you accepted to the above.\";\\n            document.getElementById(\\'acceptanceDate\\').innerText = dateString;\\n            document.getElementById(\\'content\\').classList.add(\\'accepted\\');\\n        }\\n\\n    })();\\n</script>\\n</body>\\n</html>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What?! This seems identical to the ``.text`` field. However, the careful eye would notice that the very 1st characters differ; that is, ``.content`` has a *b'* character at the beginning, which in Python syntax denotes that the data type is bytes, whereas the ``.text`` field did not have it and is a regular String.\n",
    "\n",
    "Ok, so that's great, but how do we make sense of this text? We could manually parse it, but that's tedious and difficult. As mentioned, BeautifulSoup is specifically designed to parse this exact content (any webpage content).\n",
    "\n",
    "## BEAUTIFUL SOUP\n",
    "![title](images/soup_for_you.jpg) (property of NBC)\n",
    "\n",
    "\n",
    "The [documentation for BeautifulSoup is found here](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).\n",
    "\n",
    "A BeautifulSoup object can be initialized with the ``.content`` from request and a flag denoting the type of parser that we should use. For example, we could specify ``html.parser``, ``lxml``, etc [documentation here](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#differences-between-parsers). Since we are interested in standard webpages that use HTML, let's specify the html.parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<!DOCTYPE html>\n",
       "\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "<meta content=\"noindex, nofollow\" name=\"robots\"/>\n",
       "<meta content=\"text/html;charset=utf-8\" http-equiv=\"Content-Type\"/>\n",
       "<meta content=\"utf-8\" http-equiv=\"encoding\"/>\n",
       "<meta content=\"width=device-width, initial-scale=1, minimum-scale=1, maximum-scale=1, shrink-to-fit=no\" name=\"viewport\">\n",
       "<title>NPR Choice page</title>\n",
       "<link href=\"https://s.npr.org/templates/css/fonts/Knockout.css\" media=\"screen, print\" rel=\"stylesheet\"/>\n",
       "<link href=\"https://s.npr.org/templates/css/fonts/GothamSSm.css\" media=\"screen, print\" rel=\"stylesheet\"/>\n",
       "<link href=\"css/choice-stylesheet.css\" media=\"screen, print\" rel=\"stylesheet\"/>\n",
       "<script src=\"./js/redirects.js\" type=\"text/javascript\"></script>\n",
       "<script src=\"./js/domains.js\" type=\"text/javascript\"></script>\n",
       "</meta></head>\n",
       "<body>\n",
       "<main class=\"content\" id=\"content\">\n",
       "<header role=\"banner\">\n",
       "<img alt=\"NPR logo\" class=\"npr-logo\" src=\"https://media.npr.org/chrome_svg/npr-logo.svg\"/>\n",
       "<h1 class=\"header-txt\">Data Protection Choices</h1>\n",
       "<div id=\"npr-rule\" role=\"presentation\"><span></span><span></span></div>\n",
       "</header>\n",
       "<section class=\"main-section\">\n",
       "<p>\n",
       "            By choosing “I agree” below, you agree that NPR’s sites use cookies, similar tracking and storage technologies, and information about the device you use to access our sites to enhance your viewing, listening and user experience, personalize content, personalize messages from NPR’s sponsors, provide social media features, and analyze NPR’s traffic. This information is shared with social media services, sponsorship, analytics and other third-party service providers.\n",
       "            <a href=\"https://text.npr.org/s.php?sId=609131973#cookiepolicy\">See details</a>.\n",
       "        </p>\n",
       "<p class=\"acceptance-date\" id=\"acceptanceDate\"></p>\n",
       "<div class=\"user-actions\">\n",
       "<button class=\"user-action user-action--accept\" id=\"accept\">Agree and Continue</button>\n",
       "<button class=\"user-action user-action--revoke\" id=\"revoke\">Revoke Agreement</button>\n",
       "<a class=\"user-action user-action--text\" href=\"https://text.npr.org\" id=\"textLink\">Decline and Visit Plain Text Site</a>\n",
       "</div>\n",
       "<footer class=\"footer\">\n",
       "<p>NPR’s <a href=\"https://text.npr.org/s.php?sId=179876898\">Terms of Use</a> and <a href=\"https://text.npr.org/s.php?sId=609131973\">Privacy Policy</a>.</p>\n",
       "</footer>\n",
       "</section>\n",
       "</main>\n",
       "<script>\n",
       "    // self executing function here\n",
       "    (function () {\n",
       "        var choiceVersion = 1;\n",
       "\n",
       "        // Return true is the origin param is present in the URL\n",
       "        // Make sure origin starts with \"https://\" in order to avoid cross-site scripting attack\n",
       "        var hasOrigin = function () {\n",
       "            var searchParam = window.location.search;\n",
       "            return searchParam.substr(0, 16) === '?origin=https://';\n",
       "        };\n",
       "\n",
       "        // Append choiceRedirect=true to a destination\n",
       "        // This will tell use that a user has been already redirected by the choice page\n",
       "        // stopping a potential infinite redirect loop \n",
       "        var addChoiceRedirectParam = function (url) {\n",
       "            var paramControl = '?';\n",
       "            if (url.includes('?')){\n",
       "                paramControl = '&';\n",
       "            }\n",
       "            return url + paramControl + 't=' + (new Date()).getTime();\n",
       "        }\n",
       "\n",
       "        // Redirect made from AKAMAI will include the original\n",
       "        // destination with the request ex:\n",
       "        // https://www.npr.org/choice.html?origin=https://www.npr.org/about-npr/178660742/public-radio-finances\n",
       "        var getDestination = function () {\n",
       "            var searchParam = window.location.search;\n",
       "            if (hasOrigin()) {\n",
       "                var destination = searchParam.substr(8);\n",
       "                if (checkOrigin(destination)) {\n",
       "                    return destination;\n",
       "                } \n",
       "            }\n",
       "            return 'https://www.npr.org';\n",
       "        };\n",
       "\n",
       "        var getCookie = function (name) {\n",
       "            var value = \"; \" + document.cookie;\n",
       "            var parts = value.split(\"; \" + name + \"=\");\n",
       "            if (parts.length == 2) return parts.pop().split(\";\").shift();\n",
       "            return false;\n",
       "        };\n",
       "\n",
       "        var delete_cookie = function (name) {\n",
       "            document.cookie = name + '=;expires=Thu, 01 Jan 1970 00:00:01 GMT;secure;path=/;domain=npr.org;';\n",
       "        };\n",
       "\n",
       "        var create_cookie = function (name, value) {\n",
       "            // Cookies have a tendency to expire, so I arbitrarily set the max age to 10 year\n",
       "            document.cookie = name + '=' + value + ';secure;path=/;domain=npr.org;max-age=315360000;';\n",
       "        };\n",
       "\n",
       "        // True is user previously accepted the correct version of the consent page\n",
       "        var hasPreviouslyAcceptedChoiceOptions = function () {\n",
       "            return getCookie('trackingChoice') && getCookie('choiceVersion') == choiceVersion;\n",
       "        }\n",
       "\n",
       "        // Grab the thing id form the destination\n",
       "        var getThingId = function (destination) {\n",
       "            var yearMonthDateWithPreFixReg = /https:\\/\\/www\\.npr\\.org\\/([a-z]+\\/){0,2}\\d{4}\\/\\d{2}\\/\\d{2}\\/(\\d+)\\/.*/;\n",
       "            var match = yearMonthDateWithPreFixReg.exec(destination);\n",
       "            if (match) {\n",
       "                return match[2];\n",
       "            }\n",
       "\n",
       "            var noDateUrlRegex = /https:\\/\\/www\\.npr\\.org\\/([a-z]+\\/){1,2}(\\d+)\\/.*/;\n",
       "            match = noDateUrlRegex.exec(destination);\n",
       "            if (match) {\n",
       "                return match[2];\n",
       "            }\n",
       "\n",
       "            var thingIdByParam = /https:\\/\\/www\\.npr\\.org\\/.*[iI]d=(\\d{4,}).*/;\n",
       "            match = thingIdByParam.exec(destination);\n",
       "            if (match) {\n",
       "                return match[1];\n",
       "            }\n",
       "\n",
       "            // Check if we have a hard coded page url\n",
       "            //Remove https://www.npr.org from the destination\n",
       "            var location = destination.substr(19);\n",
       "            for (var key in redirectLookup) {\n",
       "                // If the first part of the location matches a\n",
       "                // hard coded url, then we have a match.\n",
       "                if (location.startsWith(key)){\n",
       "                    return redirectLookup[key];\n",
       "                }\n",
       "            }\n",
       "\n",
       "            return false;\n",
       "        }\n",
       "\n",
       "        document.getElementById('accept').addEventListener('click', function () {\n",
       "            var d = new Date();\n",
       "            var dateOfChoice = d.getTime();\n",
       "\n",
       "            create_cookie('trackingChoice', 'true');\n",
       "            create_cookie('choiceVersion', choiceVersion);\n",
       "            create_cookie('dateOfChoice', dateOfChoice);\n",
       "            window.location = addChoiceRedirectParam(getDestination());\n",
       "        });\n",
       "\n",
       "        document.getElementById('revoke').addEventListener('click', function () {\n",
       "            delete_cookie('trackingChoice');\n",
       "            delete_cookie('choiceVersion');\n",
       "            delete_cookie('dateOfChoice');\n",
       "            document.getElementById('acceptanceDate').innerText = '';\n",
       "            document.getElementById('content').classList.remove('accepted');\n",
       "        });\n",
       "\n",
       "\n",
       "        var thingId = getThingId(getDestination());\n",
       "        if (thingId) {\n",
       "            document.getElementById('textLink').href = \"https://text.npr.org/r.php?id=\" + thingId;\n",
       "        }\n",
       "\n",
       "        if (hasOrigin() && hasPreviouslyAcceptedChoiceOptions()) {\n",
       "            // If the user has already accepted the choice options\n",
       "            // and has an origin param in his request\n",
       "            // We will redirect him to that origin request.\n",
       "            // This will solve the issue where applications are caching 307 redirects\n",
       "            window.location = addChoiceRedirectParam(getDestination());\n",
       "        } else if (hasPreviouslyAcceptedChoiceOptions()) {\n",
       "            var lastDateOfChoice = getCookie('dateOfChoice');\n",
       "            var d = new Date(parseInt(lastDateOfChoice, 10));\n",
       "            var dateString = \"On \"\n",
       "                + (d.getMonth() + 1)\n",
       "                + \"/\"\n",
       "                + (d.getDay() + 1)\n",
       "                + \"/\"\n",
       "                + d.getFullYear()\n",
       "                + \" you accepted to the above.\";\n",
       "            document.getElementById('acceptanceDate').innerText = dateString;\n",
       "            document.getElementById('content').classList.add('accepted');\n",
       "        }\n",
       "\n",
       "    })();\n",
       "</script>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! That looks a little better; there's some whitespace formatting, adding some structure to our content! HTML code is structured by `<tags>`. Every tag has an opening and closing portion, denoted by ``< >`` and ``</ >``, respectively. If we want just the text (not the tags), we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\n\\nNPR Choice page\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nData Protection Choices\\n\\n\\n\\n\\n            By choosing “I agree” below, you agree that NPR’s sites use cookies, similar tracking and storage technologies, and information about the device you use to access our sites to enhance your viewing, listening and user experience, personalize content, personalize messages from NPR’s sponsors, provide social media features, and analyze NPR’s traffic. This information is shared with social media services, sponsorship, analytics and other third-party service providers.\\n            See details.\\n        \\n\\n\\nAgree and Continue\\nRevoke Agreement\\nDecline and Visit Plain Text Site\\n\\n\\nNPR’s Terms of Use and Privacy Policy.\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's some tricky Javascript still nesting within it, but it definitely cleaned up a bit. On other websites, you may find even clearer text extraction.\n",
    "\n",
    "As detailed in the [BeautifulSoup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), the easiest way to navigate through the tags is to simply name the tag you're interested in. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<head>\n",
       "<meta content=\"noindex, nofollow\" name=\"robots\"/>\n",
       "<meta content=\"text/html;charset=utf-8\" http-equiv=\"Content-Type\"/>\n",
       "<meta content=\"utf-8\" http-equiv=\"encoding\"/>\n",
       "<meta content=\"width=device-width, initial-scale=1, minimum-scale=1, maximum-scale=1, shrink-to-fit=no\" name=\"viewport\">\n",
       "<title>NPR Choice page</title>\n",
       "<link href=\"https://s.npr.org/templates/css/fonts/Knockout.css\" media=\"screen, print\" rel=\"stylesheet\"/>\n",
       "<link href=\"https://s.npr.org/templates/css/fonts/GothamSSm.css\" media=\"screen, print\" rel=\"stylesheet\"/>\n",
       "<link href=\"css/choice-stylesheet.css\" media=\"screen, print\" rel=\"stylesheet\"/>\n",
       "<script src=\"./js/redirects.js\" type=\"text/javascript\"></script>\n",
       "<script src=\"./js/domains.js\" type=\"text/javascript\"></script>\n",
       "</meta></head>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.head # fetches the head tag, which ecompasses the title tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually head tags are small and only contain the most important contents; however, here, there's some Javascript code. The ``title`` tag resides within the head tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>NPR Choice page</title>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title # we can specifically call for the title tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result includes the tag itself. To get just the text within the tags, we can use the ``.name`` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NPR Choice page'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title.string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can navigate to the parent tag (the tag that encompasses the current tag) via the ``.parent`` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meta'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title.parent.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Parse the page with Beautiful Soup\n",
    "In HTML code, paragraphs are often denoated with a ``<p>`` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p>\n",
       "            By choosing “I agree” below, you agree that NPR’s sites use cookies, similar tracking and storage technologies, and information about the device you use to access our sites to enhance your viewing, listening and user experience, personalize content, personalize messages from NPR’s sponsors, provide social media features, and analyze NPR’s traffic. This information is shared with social media services, sponsorship, analytics and other third-party service providers.\n",
       "            <a href=\"https://text.npr.org/s.php?sId=609131973#cookiepolicy\">See details</a>.\n",
       "        </p>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns the first paragraph, and we can access properties of the given tag with the same syntax we use for dictionaries and dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6ccd75acfeac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/bs4/element.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1399\u001b[0m         \"\"\"tag[key] returns the value of the 'key' attribute for the Tag,\n\u001b[1;32m   1400\u001b[0m         and throws an exception if it's not there.\"\"\"\n\u001b[0;32m-> 1401\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'class'"
     ]
    }
   ],
   "source": [
    "soup.p['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to 'paragraph' (aka p) tags, link tags are also very common and are denoted by ``<a>`` tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"https://text.npr.org/s.php?sId=609131973#cookiepolicy\">See details</a>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is called the a tag because links are also called 'anchors'. Nearly every page has multiple paragraphs and anchors, so how do we access the subsequent tags? There are two common functions, `.find()` and `.find_all()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>NPR Choice page</title>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<title>NPR Choice page</title>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the results were seemingly the same, since there is only one title to a webpage. However, you'll notice that ``.find_all()`` returned a list, not a single item. Sure, there was only one item in the list, but it returned a list. As the name implies, find_all() returns all items that match the passed-in tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"https://text.npr.org/s.php?sId=609131973#cookiepolicy\">See details</a>,\n",
       " <a class=\"user-action user-action--text\" href=\"https://text.npr.org\" id=\"textLink\">Decline and Visit Plain Text Site</a>,\n",
       " <a href=\"https://text.npr.org/s.php?sId=179876898\">Terms of Use</a>,\n",
       " <a href=\"https://text.npr.org/s.php?sId=609131973\">Privacy Policy</a>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at all of those links! Amazing. It might be hard to read but the **href** portion of an *a* tag denotes the URL, and we can capture it via the ``.get()`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://text.npr.org/s.php?sId=609131973#cookiepolicy\n",
      "https://text.npr.org\n",
      "https://text.npr.org/s.php?sId=179876898\n",
      "https://text.npr.org/s.php?sId=609131973\n"
     ]
    }
   ],
   "source": [
    "for link in soup.find_all('a'): # we could optionally pass the href=True flag .find_all('a', href=True)\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of those links are relative to the current URL (e.g., /section/news/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p>\n",
       "             By choosing “I agree” below, you agree that NPR’s sites use cookies, similar tracking and storage technologies, and information about the device you use to access our sites to enhance your viewing, listening and user experience, personalize content, personalize messages from NPR’s sponsors, provide social media features, and analyze NPR’s traffic. This information is shared with social media services, sponsorship, analytics and other third-party service providers.\n",
       "             <a href=\"https://text.npr.org/s.php?sId=609131973#cookiepolicy\">See details</a>.\n",
       "         </p>,\n",
       " <p class=\"acceptance-date\" id=\"acceptanceDate\"></p>,\n",
       " <p>NPR’s <a href=\"https://text.npr.org/s.php?sId=179876898\">Terms of Use</a> and <a href=\"https://text.npr.org/s.php?sId=609131973\">Privacy Policy</a>.</p>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs = soup.find_all('p')\n",
    "paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want just the paragraph text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            By choosing “I agree” below, you agree that NPR’s sites use cookies, similar tracking and storage technologies, and information about the device you use to access our sites to enhance your viewing, listening and user experience, personalize content, personalize messages from NPR’s sponsors, provide social media features, and analyze NPR’s traffic. This information is shared with social media services, sponsorship, analytics and other third-party service providers.\n",
      "            See details.\n",
      "        \n",
      "\n",
      "NPR’s Terms of Use and Privacy Policy.\n"
     ]
    }
   ],
   "source": [
    "for pa in paragraphs:\n",
    "    print(pa.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are multiple tags and various attributes, it is useful to check the data type of BeautifulSoup objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(soup.find('p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ``.find()`` function returns a BeautifulSoup element, we can tack on multiple calls that continue to return elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p>\n",
       "            By choosing “I agree” below, you agree that NPR’s sites use cookies, similar tracking and storage technologies, and information about the device you use to access our sites to enhance your viewing, listening and user experience, personalize content, personalize messages from NPR’s sponsors, provide social media features, and analyze NPR’s traffic. This information is shared with social media services, sponsorship, analytics and other third-party service providers.\n",
       "            <a href=\"https://text.npr.org/s.php?sId=609131973#cookiepolicy\">See details</a>.\n",
       "        </p>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"https://text.npr.org/s.php?sId=609131973#cookiepolicy\">See details</a>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('p').find('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://text.npr.org/s.php?sId=609131973#cookiepolicy'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('p').find('a').attrs['href'] # att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'See details'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('p').find('a').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't look pretty, but it makes sense because if you look at what ``.find('a')`` returned, there is plenty of whitespace. We can remove that with Python's built-in ``.strip()`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'See details'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('p').find('a').text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** above, we accessed the attributes of a link by using the property ``.attrs``. ``.attrs`` takes a dictionary as a parameter, and in the example above, we only provided the _key_, not a _value_, too. That is, we only cared that the ``<a>`` tag had an attribute named ``href`` (which we grabbed by typing that command), and we made no specific demands on what the value must be. In other words, regardless of the value of _href_, we grabbed that element. Alternatively, if you inspect your HTML code and notice select regions for which you'd like to extract text, you can specify it as part of the attributes, too!\n",
    "\n",
    "For example, in the full ``response.text``, we see the following line:\n",
    "\n",
    "``<header class=\"npr-header\" id=\"globalheader\" aria-label=\"NPR header\">``\n",
    "\n",
    "Let's say that we know that the information we care about is within tags that match this template (i.e., **class** is an attribute, and its value is **'npr-header'**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('header', attrs={'class':'npr-header'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matched it! We could then continue further processing by tacking on other commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-2caf48eb3508>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'header'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'npr-header'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"li\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# li stands for list items\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "soup.find('header', attrs={'class':'npr-header'}).find_all(\"li\") # li stands for list items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns all of our list items, and since it's within a particular header section of the page, it appears they are links to menu items for navigating the webpage. If we wanted to grab just the links within these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-8bbedc0273a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmenu_links\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mlist_item\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'header'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'npr-header'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"li\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mmenu_links\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmenu_links\u001b[0m \u001b[0;31m# a unique set of all the seemingly important links in the header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "menu_links = set()\n",
    "for list_item in soup.find('header', attrs={'class':'npr-header'}).find_all(\"li\"):\n",
    "    for link in list_item.find_all('a', href=True):\n",
    "        menu_links.add(link)\n",
    "menu_links # a unique set of all the seemingly important links in the header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAKEAWAY LESSON\n",
    "The above tutorial isn't meant to be a study guide to memorize; its point is to show you the most important functionaity that exist within BeautifulSoup, and to illustrate how one can access different pieces of content. No two web scraping tasks are identical, so it's useful to play around with code and try different things, while using the above as examples of how you may navigate between different tags and properties of a page. Don't worry; we are always here to help when you get stuck!\n",
    "\n",
    "# String formatting\n",
    "As we parse webpages, we may often want to further adjust and format the text to a certain way.\n",
    "\n",
    "For example, say we wanted to scrape a polical website that lists all US Senators' name and office phone number. We may want to store information for each senator in a dictionary. All senators' information may be stored in a list. Thus, we'd have a list of dictionaries. Below, we will initialize such a list of dictionary (it has only 3 senators, for illustrative purposes, but imagine it contains many more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'Lamar Alexander', 'number': '555-229-2812'}, {'name': 'Tammy Baldwin', 'number': '555-922-8393'}, {'name': 'John Barrasso', 'number': '555-827-2281'}]\n"
     ]
    }
   ],
   "source": [
    "# this is a bit clumsy of an initialization, but we spell it out this way for clarity purposes\n",
    "# NOTE: imagine the dictionary were constructed in a more organic manner\n",
    "senator1 = {\"name\":\"Lamar Alexander\", \"number\":\"555-229-2812\"}\n",
    "senator2 = {\"name\":\"Tammy Baldwin\", \"number\":\"555-922-8393\"}\n",
    "senator3 = {\"name\":\"John Barrasso\", \"number\":\"555-827-2281\"}\n",
    "senators = [senator1, senator2, senator3]\n",
    "print(senators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the real-world, we may not want the final form of our information to be in a Python dictionary; rather, we may need to send an email to people in our mailing list, urging them to call their senators. If we have a templated format in mind, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please call Lamar Alexander at 555-229-2812\n",
      "Please call Tammy Baldwin at 555-922-8393\n",
      "Please call John Barrasso at 555-827-2281\n"
     ]
    }
   ],
   "source": [
    "email_template = \"\"\"Please call {name} at {number}\"\"\"\n",
    "for senator in senators:\n",
    "    print(email_template.format(**senator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please [visit here](https://docs.python.org/3/library/stdtypes.html#str.format)** for further documentation\n",
    "                      \n",
    "Alternatively, one can also format their text via the ``f'-strings`` property. [See documentation here](https://docs.python.org/3/reference/lexical_analysis.html#f-strings). For example, using the above data structure and goal, one could yield identical results via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please call Lamar Alexander at 555-229-2812\n",
      "Please call Tammy Baldwin at 555-922-8393\n",
      "Please call John Barrasso at 555-827-2281\n"
     ]
    }
   ],
   "source": [
    "for senator in senators:\n",
    "    print(f\"Please call {senator['name']} at {senator['number']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, sometimes we wish to search large strings of text. If we wish to find all occurrences within a given string, a very mechanical, procedural way of doing it would be to use the ``.find()`` function in Python and to repeatedly update the starting index from which we are looking.\n",
    "\n",
    "## Regular Expressions\n",
    "A way more suitable and powerful way is to use Regular Expressions, which is a pattern matching mechanism used throughout Computer Science and programming (it's not just specific to Python). A tutorial on Regular Expressions (aka regex) is beond this lab, but below are many great resources that we recommend, if you are interested in them (could be very useful for a homework problem):\n",
    "- https://docs.python.org/3.3/library/re.html\n",
    "- https://regexone.com\n",
    "- https://docs.python.org/3/howto/regex.html.\n",
    "\n",
    "# Additonal Python/Homework Comment\n",
    "In Homework #1, we ask you to complete functions that have signatures with a syntax you may not have seen before:\n",
    "\n",
    "``def create_star_table(starlist: list) -> list:``\n",
    "\n",
    "To be clear, this syntax merely means that the input parameter must be a list, and the output must be a list. It's no different than any other function, it just puts a requirement on the behavior of the function.\n",
    "\n",
    "It is **typing** our function. Please [see this documention if you have more questions.](https://docs.python.org/3/library/typing.html)\n",
    "\n",
    "# Walkthrough Example (of Web Scraping)\n",
    "We're going to see the structure of Goodread's best books list (**NOTE: Goodreads is described a little more within the other Lab2_More_Pandas.ipynb notebook)**. We'll use the Developer tools in chrome, safari and firefox have similar tools available. To get this page we use the `requests` module. But first we should check if the company's policy allows scraping. Check the [robots.txt](https://www.goodreads.com/robots.txt) to find what sites/elements are not accessible. Please read and verify.\n",
    "\n",
    "![](images/goodreads1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>NPR Choice page</title>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=\"https://www.npr.org/2018/11/05/664395755/what-if-the-polls-are-wrong-again-4-scenarios-for-what-might-happen-in-the-elect\"\n",
    "response = requests.get(url)\n",
    "# response.status_code\n",
    "# response.content\n",
    "\n",
    "# Beautiful Soup (library) time!\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    #print(soup)\n",
    "    # soup.prettify()\n",
    "soup.find(\"title\")\n",
    "\n",
    "    # Q1: how do we get the title's text?\n",
    "\n",
    "    # Q2: how do we get the webpage's entire content?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.goodreads.com/list/show/1.Best_Books_Ever?page=1\n"
     ]
    }
   ],
   "source": [
    "URLSTART=\"https://www.goodreads.com\"\n",
    "BESTBOOKS=\"/list/show/1.Best_Books_Ever?page=\"\n",
    "url = URLSTART+BESTBOOKS+'1'\n",
    "print(url)\n",
    "page = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see properties of the page. Most relevant are `status_code` and `text`. The former tells us  if the web-page was found, and if found , ok. (See lecture notes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.status_code # 200 is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html class=\"desktop\\n\">\\n<head>\\n  <title>Best Books Ever (52732 books)</title>\\n\\n<meta content=\\'51,239 books based on 200692 votes: The Hunger Games by Suzanne Collins, Harry Potter and the Order of the Phoenix by J.K. Rowling, To Kill a Mockingbird...\\' name=\\'description\\'>\\n<meta content=\\'telephone=no\\' name=\\'format-detection\\'>\\n<link href=\\'https://www.goodreads.com/list/show/1.Best_Books_Ever\\' rel=\\'canonical\\'>\\n\\n\\n\\n    <script type=\"text/javascript\"> var ue_t0=window.ue_t0||+new Date();\\n </script>\\n  <script type=\"text/javascript\">\\n    var ue_mid = \"A1PQBFHBHS6YH1\";\\n    var ue_sn = \"www.goodreads.com\";\\n    var ue_furl = \"fls-na.amazon.com\";\\n    var ue_sid = \"929-5702512-1555776\";\\n    var ue_id = \"1BFVDCB0SERV4FAMASZE\";\\n\\n    (function(e){var c=e;var a=c.ue||{};a.main_scope=\"mainscopecsm\";a.q=[];a.t0=c.ue_t0||+new Date();a.d=g;function g(h){return +new Date()-(h?0:a.t0)}function d(h){return function(){a.q.push({n:h,a:arguments,t:a.d()})}}function b(m,l,h,j,i){var k={m:m,f:l,l:h,c:\"\"+j,err:i,fromOnError:1,args:arguments};c.ueLogError(k);return false}b.skipTrace=1;e.onerror=b;function f(){c.uex(\"ld\")}if(e.addEventListener){e.addEventListener(\"load\",f,false)}else{if(e.attachEvent){e.attachEvent(\"onload\",f)}}a.tag=d(\"tag\");a.log=d(\"log\");a.reset=d(\"rst\");c.ue_csm=c;c.ue=a;c.ueLogError=d(\"err\");c.ues=d(\"ues\");c.uet=d(\"uet\");c.uex=d(\"uex\");c.uet(\"ue\")})(window);(function(e,d){var a=e.ue||{};function c(g){if(!g){return}var f=d.head||d.getElementsByTagName(\"head\")[0]||d.documentElement,h=d.createElement(\"script\");h.async=\"async\";h.src=g;f.insertBefore(h,f.firstChild)}function b(){var k=e.ue_cdn||\"z-ecx.images-amazon.com\",g=e.ue_cdns||\"images-na.ssl-images-amazon.com\",j=\"/images/G/01/csminstrumentation/\",h=e.ue_file||\"ue-full-11e51f253e8ad9d145f4ed644b40f692._V1_.js\",f,i;if(h.indexOf(\"NSTRUMENTATION_FIL\")>=0){return}if(\"ue_https\" in e){f=e.ue_https}else{f=e.location&&e.location.protocol==\"https:\"?1:0}i=f?\"https://\":\"http://\";i+=f?g:k;i+=j;i+=h;c(i)}if(!e.ue_inline){if(a.loadUEFull){a.loadUEFull()}else{b()}}a.uels=c;e.ue=a})(window,document);\\n\\n    if (window.ue && window.ue.tag) { window.ue.tag(\\'list:show:signed_out\\', ue.main_scope);window.ue.tag(\\'list:show:signed_out:desktop\\', ue.main_scope); }\\n  </script>\\n\\n\\n          <script type=\"text/javascript\">\\n        if (window.Mobvious === undefined) {\\n          window.Mobvious = {};\\n        }\\n        window.Mobvious.device_type = \\'desktop\\';\\n        </script>\\n\\n\\n  \\n<script src=\"https://s.gr-assets.com/assets/webfontloader-032396443d87656441bab7d02193984c.js\"></script>\\n<script>\\n//<![CDATA[\\n\\n  WebFont.load({\\n    classes: false,\\n    custom: {\\n      families: [\"Lato:n4,n7,i4\", \"Merriweather:n4,n7,i4\"],\\n      urls: [\"https://s.gr-assets.com/assets/gr/fonts-e256f84093cc13b27f5b82343398031a.css\"]\\n    }\\n  });\\n\\n//]]>\\n</script>\\n\\n  <link rel=\"stylesheet\" media=\"all\" href=\"https://s.gr-assets.com/assets/goodreads-f1effd616c9a8b53694656e2c63b3060.css\" />\\n\\n    <style type=\"text/css\" media=\"screen\">\\n    .bigTabs {\\n      margin-bottom: 10px;\\n    }\\n\\n    .list_read{\\n      background-color: #D7D2C4;\\n      float: left;\\n    }\\n  </style>\\n\\n\\n  <link rel=\"stylesheet\" media=\"screen\" href=\"https://s.gr-assets.com/assets/common_images-670d97636259cafc355c94fc43e871d7.css\" />\\n\\n  <script src=\"https://s.gr-assets.com/assets/desktop/libraries-c3b90bb3e1f8d740915ab22f13b11359.js\"></script>\\n  <script src=\"https://s.gr-assets.com/assets/application-c529f9fb2f17ff0d0cbb83f99171f460.js\"></script>\\n\\n    <script>\\n  //<![CDATA[\\n    var gptAdSlots = gptAdSlots || [];\\n    var googletag = googletag || {};\\n    googletag.cmd = googletag.cmd || [];\\n    (function() {\\n      var gads = document.createElement(\"script\");\\n      gads.async = true;\\n      gads.type = \"text/javascript\";\\n      var useSSL = \"https:\" == document.location.protocol;\\n      gads.src = (useSSL ? \"https:\" : \"http:\") +\\n      \"//securepubads.g.doubleclick.net/tag/js/gpt.js\";\\n      var node = document.getElementsByTagName(\"script\")[0];\\n      node.parentNode.insertBefore(gads, node);\\n    })();\\n    // page settings\\n  //]]>\\n</script>\\n<script>\\n  //<![CDATA[\\n    googletag.cmd.push(function() {\\n      googletag.pubads().setTargeting(\"sid\", \"osid.2c8e1fced4d3db5843adff5f143b361b\");\\n    googletag.pubads().setTargeting(\"grsession\", \"osid.2c8e1fced4d3db5843adff5f143b361b\");\\n    googletag.pubads().setTargeting(\"surface\", \"desktop\");\\n    googletag.pubads().setTargeting(\"signedin\", \"false\");\\n    googletag.pubads().setTargeting(\"gr_author\", \"false\");\\n    googletag.pubads().setTargeting(\"author\", []);\\n    googletag.pubads().setTargeting(\"shelf\", [\"best\",\"earliestlist\",\"favourites\",\"fiction\",\"writer\"]);\\n    googletag.pubads().setTargeting(\"gtargeting\", \"1z141z5\");\\n    googletag.pubads().setTargeting(\"resource\", \"List_1\");\\n      googletag.pubads().enableAsyncRendering();\\n      googletag.pubads().enableSingleRequest();\\n      googletag.pubads().collapseEmptyDivs(true);\\n      googletag.pubads().disableInitialLoad();\\n      googletag.enableServices();\\n    });\\n  //]]>\\n</script'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.text[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us write a loop to fetch 2 pages of \"best-books\" from goodreads. Notice the use of a format string. This is an example of old-style python format strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FTW files/page01.html\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'files/page01.html'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-491c49c8ce3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfiletowrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"files/page\"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m'%02d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".html\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FTW\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiletowrite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mfd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiletowrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mfd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstuff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mfd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'files/page01.html'"
     ]
    }
   ],
   "source": [
    "URLSTART=\"https://www.goodreads.com\"\n",
    "BESTBOOKS=\"/list/show/1.Best_Books_Ever?page=\"\n",
    "for i in range(1,3):\n",
    "    bookpage=str(i)\n",
    "    stuff=requests.get(URLSTART+BESTBOOKS+bookpage)\n",
    "    filetowrite=\"files/page\"+ '%02d' % i + \".html\"\n",
    "    print(\"FTW\", filetowrite)\n",
    "    fd=open(filetowrite,\"w\")\n",
    "    fd.write(stuff.text)\n",
    "    fd.close()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parse the page, extract book urls\n",
    "\n",
    "Notice how we do file input-output, and use beautiful soup in the code below. The `with` construct ensures that the file being read is closed, something we do explicitly for the file being written. We look for the elements with class `bookTitle`, extract the urls, and write them into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookdict={}\n",
    "for i in range(1,3):\n",
    "    books=[]\n",
    "    stri = '%02d' % i\n",
    "    filetoread=\"files/page\"+ stri + '.html'\n",
    "    print(\"FTW\", filetoread)\n",
    "    with open(filetoread) as fdr:\n",
    "        data = fdr.read()\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    for e in soup.select('.bookTitle'):\n",
    "        books.append(e['href'])\n",
    "    print(books[:10])\n",
    "    bookdict[stri]=books\n",
    "    fd=open(\"files/list\"+stri+\".txt\",\"w\")\n",
    "    fd.write(\"\\n\".join(books))\n",
    "    fd.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is George Orwell's 1984"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookdict['02'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Lets go look at the first URLs on both pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/goodreads2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parse a book page, extract book properties\n",
    "\n",
    "Ok so now lets dive in and get one of these these files and parse them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bookdict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-2b058857671b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfurl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mURLSTART\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbookdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'02'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfurl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bookdict' is not defined"
     ]
    }
   ],
   "source": [
    "furl=URLSTART+bookdict['02'][0]\n",
    "furl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/goodreads3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'furl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-1d5eadeebd6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfstuff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfurl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfstuff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'furl' is not defined"
     ]
    }
   ],
   "source": [
    "fstuff=requests.get(furl)\n",
    "print(fstuff.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fstuff' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-a3eadb7193b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#d=BeautifulSoup(fstuff.text, 'html.parser')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# try this to take care of arabic strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfstuff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_encoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'fstuff' is not defined"
     ]
    }
   ],
   "source": [
    "#d=BeautifulSoup(fstuff.text, 'html.parser')\n",
    "# try this to take care of arabic strings\n",
    "d = BeautifulSoup(fstuff.text, 'html.parser', from_encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-e43b7245c5f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"meta[property='og:title']\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "d.select(\"meta[property='og:title']\")[0]['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get everything we want..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-fb1816a4cf90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#d=BeautifulSoup(fstuff.text, 'html.parser', from_encoding=\"utf-8\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m print(\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;34m\"title\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"meta[property='og:title']\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\"isbn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"meta[property='books:isbn']\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\"type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"meta[property='og:type']\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "#d=BeautifulSoup(fstuff.text, 'html.parser', from_encoding=\"utf-8\")\n",
    "print(\n",
    "\"title\", d.select_one(\"meta[property='og:title']\")['content'],\"\\n\",\n",
    "\"isbn\", d.select(\"meta[property='books:isbn']\")[0]['content'],\"\\n\",\n",
    "\"type\", d.select(\"meta[property='og:type']\")[0]['content'],\"\\n\",\n",
    "\"author\", d.select(\"meta[property='books:author']\")[0]['content'],\"\\n\",\n",
    "#\"average rating\", d.select_one(\"span.average\").text,\"\\n\",\n",
    "\"ratingCount\", d.select(\"meta[itemprop='ratingCount']\")[0][\"content\"],\"\\n\"\n",
    "#\"reviewCount\", d.select_one(\"span.count\")[\"title\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we know what to do, lets wrap our fetching into a proper script. So that we dont overwhelm their servers, we will only fetch 5 from each page, but you get the idea...\n",
    "\n",
    "We'll segue of a bit to explore new style format strings. See https://pyformat.info for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'list03.txt'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"list{:0>2}.txt\".format(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"4\"\n",
    "b = 4\n",
    "class Four:\n",
    "    def __str__(self):\n",
    "        return \"Fourteen\"\n",
    "c=Four()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The hazy cat jumped over the 4 and 4 and Fourteen'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"The hazy cat jumped over the {} and {} and {}\".format(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set up a pipeline for fetching and parsing\n",
    "\n",
    "Ok lets get back to the fetching..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'files/list01.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-85132e9b8723>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"files/list{:0>2}.txt\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mcounter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbookurl_line\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'files/list01.txt'"
     ]
    }
   ],
   "source": [
    "fetched=[]\n",
    "for i in range(1,3):\n",
    "    with open(\"files/list{:0>2}.txt\".format(i)) as fd:\n",
    "        counter=0\n",
    "        for bookurl_line in fd:\n",
    "            if counter > 4:\n",
    "                break\n",
    "            bookurl=bookurl_line.strip()\n",
    "            stuff=requests.get(URLSTART+bookurl)\n",
    "            filetowrite=bookurl.split('/')[-1]\n",
    "            filetowrite=\"files/\"+str(i)+\"_\"+filetowrite+\".html\"\n",
    "            print(\"FTW\", filetowrite)\n",
    "            fd=open(filetowrite,\"w\")\n",
    "            fd.write(stuff.text)\n",
    "            fd.close()\n",
    "            fetched.append(filetowrite)\n",
    "            time.sleep(2)\n",
    "            counter=counter+1\n",
    "            \n",
    "print(fetched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok we are off to parse each one of the html pages we fetched. We have provided the skeleton of the code and the code to parse the year, since it is a bit more complex...see the difference in the screenshots above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "yearre = r'\\d{4}'\n",
    "def get_year(d):\n",
    "    if d.select_one(\"nobr.greyText\"):\n",
    "        return d.select_one(\"nobr.greyText\").text.strip().split()[-1][:-1]\n",
    "    else:\n",
    "        thetext=d.select(\"div#details div.row\")[1].text.strip()\n",
    "        rowmatch=re.findall(yearre, thetext)\n",
    "        if len(rowmatch) > 0:\n",
    "            rowtext=rowmatch[0].strip()\n",
    "        else:\n",
    "            rowtext=\"NA\"\n",
    "        return rowtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "\n",
    "Your job is to fill in the code to get the genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_genres(d):\n",
    "    # your code here\n",
    "    genres=d.select(\"div.elementList div.left a\")\n",
    "    glist=[]\n",
    "    for g in genres:\n",
    "        glist.append(g['href'])\n",
    "    return glist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "listofdicts=[]\n",
    "for filetoread in fetched:\n",
    "    print(filetoread)\n",
    "    td={}\n",
    "    with open(filetoread) as fd:\n",
    "        datext = fd.read()\n",
    "    d=BeautifulSoup(datext, 'html.parser')\n",
    "    td['title']=d.select_one(\"meta[property='og:title']\")['content']\n",
    "    td['isbn']=d.select_one(\"meta[property='books:isbn']\")['content']\n",
    "    td['booktype']=d.select_one(\"meta[property='og:type']\")['content']\n",
    "    td['author']=d.select_one(\"meta[property='books:author']\")['content']\n",
    "    #td['rating']=d.select_one(\"span.average\").text\n",
    "    td['year'] = get_year(d)\n",
    "    td['file']=filetoread\n",
    "    glist = get_genres(d)\n",
    "    td['genres']=\"|\".join(glist)\n",
    "    listofdicts.append(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-17f5422dfc52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlistofdicts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "listofdicts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally lets write all this stuff into a csv file which we will use to do analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_records(listofdicts)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'files/meta_utf8_EK.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-1d67aa589138>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"files/meta_utf8_EK.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors)\u001b[0m\n\u001b[1;32m   3165\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3166\u001b[0m         )\n\u001b[0;32m-> 3167\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m                 \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m             )\n\u001b[1;32m    192\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;31m# No explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'files/meta_utf8_EK.csv'"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"files/meta_utf8_EK.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
